{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d582b14e-35e1-4439-bbe3-eb0608d63c79",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Script Modificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ea1811-a74c-4489-944b-86c4c3a340a4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-22T00:00:40.8086753Z",
       "execution_start_time": "2025-03-22T00:00:38.1322834Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "068b21e8-e6b7-4aaa-81f1-a5939f4e2114",
       "queued_time": "2025-03-22T00:00:26.9846842Z",
       "session_id": "d4a5bc0d-7cf5-4ec8-8634-dbbe7d1b5f1c",
       "session_start_time": "2025-03-22T00:00:26.98602Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, d4a5bc0d-7cf5-4ec8-8634-dbbe7d1b5f1c, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Librerias a usar\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql.functions import col, when, lit, to_date, concat, lpad\n",
    "from io import StringIO\n",
    "from pyspark.sql.functions import col\n",
    "from functools import reduce\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id, col, max as spark_max\n",
    "requests.packages.urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be383476-0bdf-4393-a58c-daae58c0f8e4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-22T00:00:41.0863504Z",
       "execution_start_time": "2025-03-22T00:00:40.811485Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "65b1f2a5-3638-4929-9a33-b8d0fd921adb",
       "queued_time": "2025-03-22T00:00:26.9905913Z",
       "session_id": "d4a5bc0d-7cf5-4ec8-8634-dbbe7d1b5f1c",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 4,
       "statement_ids": [
        4
       ]
      },
      "text/plain": [
       "StatementMeta(, d4a5bc0d-7cf5-4ec8-8634-dbbe7d1b5f1c, 4, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definir periodo de tiempo\n",
    "start_year = 2020\n",
    "current_year = 2025\n",
    "years = list(range(start_year, current_year + 1))\n",
    "ANOS_ACTUALIZAR = [2023, 2024, 2025]\n",
    "\n",
    "# Ruta de la tabla Hechos\n",
    "ruta_hechos = \"Tables/Hechos\"\n",
    "\n",
    "#Definir columnas de análisis\n",
    "columnas_requeridas = [\n",
    "    'ANO_EJE', 'MES_EJE', 'NIVEL_GOBIERNO_NOMBRE', 'SECTOR_NOMBRE',\n",
    "    'PLIEGO_NOMBRE', 'EJECUTORA_NOMBRE', 'DEPARTAMENTO_EJECUTORA_NOMBRE',\n",
    "    'PROVINCIA_EJECUTORA_NOMBRE', 'DISTRITO_EJECUTORA_NOMBRE',\n",
    "    'PROGRAMA_PPTO_NOMBRE', 'TIPO_ACT_PROY_NOMBRE',\n",
    "    'PRODUCTO_PROYECTO_NOMBRE', 'ACTIVIDAD_ACCION_OBRA_NOMBRE', 'MONTO_PIA',\n",
    "    'MONTO_PIM', 'MONTO_CERTIFICADO', 'MONTO_COMPROMETIDO_ANUAL',\n",
    "    'MONTO_COMPROMETIDO', 'MONTO_DEVENGADO', 'MONTO_GIRADO'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24419e26-a415-4d11-8a40-3b3d274e4d90",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-22T00:01:03.8191024Z",
       "execution_start_time": "2025-03-22T00:00:41.0891644Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "490b53b5-3a02-49c5-9049-1c45726bfeb7",
       "queued_time": "2025-03-22T00:00:26.9913749Z",
       "session_id": "d4a5bc0d-7cf5-4ec8-8634-dbbe7d1b5f1c",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(, d4a5bc0d-7cf5-4ec8-8634-dbbe7d1b5f1c, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 La tabla Hechos ya existe. Solo se descargarán los datos de 2023, 2024 y 2025.\n",
      "Datos de 2023 descargados correctamente. Registros: 35426\n",
      "Datos de 2024 descargados correctamente. Registros: 3302\n",
      "Datos de 2025 descargados correctamente. Registros: 370\n"
     ]
    }
   ],
   "source": [
    "# Verificar si la tabla ya existe\n",
    "try:\n",
    "    hechos_delta = DeltaTable.forPath(spark, ruta_hechos)\n",
    "    tabla_existe = True\n",
    "    print(\"📌 La tabla Hechos ya existe. Solo se descargarán los datos de 2023, 2024 y 2025.\")\n",
    "    years = ANOS_ACTUALIZAR  # Limitar la descarga a estos años\n",
    "except AnalysisException:\n",
    "    print(\"⚠️ La tabla Hechos aún no existe. Se procesarán todos los años desde 2020.\")\n",
    "    tabla_existe = False\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for year in years:\n",
    "    url_csv = f'https://fs.datosabiertos.mef.gob.pe/datastorefiles/{year}-Gasto-COVID-19.csv'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url_csv, verify=False, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Leer CSV en Pandas\n",
    "        df_pandas = pd.read_csv(StringIO(response.text), usecols=columnas_requeridas, low_memory=False)\n",
    "        \n",
    "        # Convertir Pandas a Spark DataFrame\n",
    "        df_spark = spark.createDataFrame(df_pandas)\n",
    "        \n",
    "        dfs.append(df_spark)\n",
    "        print(f\"Datos de {year} descargados correctamente. Registros: {df_spark.count()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al descargar {year}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7cfc13d-40f3-4cb3-bd63-15718e9c99a9",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-22T00:01:04.6931607Z",
       "execution_start_time": "2025-03-22T00:01:03.822081Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "9e4a6158-524b-4880-b4f5-826312cb3028",
       "queued_time": "2025-03-22T00:00:26.9920554Z",
       "session_id": "d4a5bc0d-7cf5-4ec8-8634-dbbe7d1b5f1c",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, d4a5bc0d-7cf5-4ec8-8634-dbbe7d1b5f1c, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos totales obtenidos: 39098 registros.\n"
     ]
    }
   ],
   "source": [
    "# Unir todos los dataframes\n",
    "if dfs:\n",
    "    df = reduce(lambda df1, df2: df1.union(df2), dfs)\n",
    "    print(f\"Datos totales obtenidos: {df.count()} registros.\")\n",
    "\n",
    "# Ver estructura\n",
    "#df.printSchema()\n",
    "\n",
    "# Ver registros\n",
    "#display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7da653da-8594-4fb4-b3d5-8584cd581101",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-22T00:01:05.0219984Z",
       "execution_start_time": "2025-03-22T00:01:04.6957625Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "abc848dc-1b01-47c0-88e3-26f380c1b785",
       "queued_time": "2025-03-22T00:00:26.9926678Z",
       "session_id": "d4a5bc0d-7cf5-4ec8-8634-dbbe7d1b5f1c",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 7,
       "statement_ids": [
        7
       ]
      },
      "text/plain": [
       "StatementMeta(, d4a5bc0d-7cf5-4ec8-8634-dbbe7d1b5f1c, 7, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_or_update_dim_table(df, columns, id_column, table_path):    \n",
    "    # Seleccionar y eliminar duplicados en los datos nuevos\n",
    "    df_dim = df.select(*columns).dropDuplicates()\n",
    "\n",
    "    # Agregar columnas solo para DimTiempo\n",
    "    if \"DimTiempo\" in table_path:\n",
    "        df_dim = df_dim.withColumn(\n",
    "            \"Es_Presupuesto_Inicial\", when(col(\"MES_EJE\") == 0, lit(True)).otherwise(lit(False))\n",
    "        ).withColumn(\n",
    "            \"Fecha_Referencia\",\n",
    "            when(\n",
    "                col(\"MES_EJE\") == 0,\n",
    "                to_date(concat(col(\"ANO_EJE\").cast(\"string\"), lit(\"-01-01\")), \"yyyy-MM-dd\")\n",
    "            ).otherwise(\n",
    "                to_date(concat(col(\"ANO_EJE\").cast(\"string\"), lit(\"-\"), lpad(col(\"MES_EJE\").cast(\"string\"), 2, \"0\"), lit(\"-01\")), \"yyyy-MM-dd\")\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Revisar si la Delta Table ya existe\n",
    "    try:\n",
    "        delta_table = DeltaTable.forPath(spark, table_path)  # Cargar la tabla Delta existente\n",
    "        existing_df = delta_table.toDF().select(*columns).dropDuplicates()  # Mantener solo las columnas relevantes\n",
    "\n",
    "        # Filtrar solo los registros nuevos\n",
    "        df_new = df_dim.join(existing_df, columns, \"left_anti\")  \n",
    "\n",
    "        if df_new.isEmpty():\n",
    "            print(f\"✅ No hay datos nuevos para insertar en {table_path}\")\n",
    "            return\n",
    "\n",
    "        # Obtener el último ID generado\n",
    "        last_id = delta_table.toDF().select(spark_max(id_column)).collect()[0][0]\n",
    "        last_id = last_id if last_id is not None else 0\n",
    "\n",
    "        # Asignar IDs solo a los nuevos datos\n",
    "        window_spec = Window.orderBy(monotonically_increasing_id())\n",
    "        df_new = df_new.withColumn(id_column, row_number().over(window_spec) + last_id)\n",
    "\n",
    "        # Escribir solo los datos nuevos\n",
    "        delta_table.alias(\"existing\").merge(\n",
    "            df_new.alias(\"new\"),\n",
    "            \" AND \".join([f\"existing.{col} = new.{col}\" for col in columns])\n",
    "        ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "        print(f\"🔄 {df_new.count()} registros nuevos insertados en {table_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Si la tabla no existe, crearla desde cero\n",
    "        df_dim = df_dim.withColumn(id_column, row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "        df_dim.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
    "        \n",
    "        print(f\"✅ Creación exitosa en {table_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01d07ba2-9d07-4aa7-813b-c2fcaaedbacb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-22T00:01:23.6349378Z",
       "execution_start_time": "2025-03-22T00:01:05.0245672Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "d7cb09e4-b65f-4e88-9379-c65bc91d985d",
       "queued_time": "2025-03-22T00:00:26.9932401Z",
       "session_id": "d4a5bc0d-7cf5-4ec8-8634-dbbe7d1b5f1c",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        8
       ]
      },
      "text/plain": [
       "StatementMeta(, d4a5bc0d-7cf5-4ec8-8634-dbbe7d1b5f1c, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No hay datos nuevos para insertar en Tables/DimTiempo\n",
      "✅ No hay datos nuevos para insertar en Tables/DimUbicacion\n",
      "✅ No hay datos nuevos para insertar en Tables/DimEntidad\n",
      "✅ No hay datos nuevos para insertar en Tables/DimActividad\n"
     ]
    }
   ],
   "source": [
    "# Columnas de cada dimensión\n",
    "dim_tables = [\n",
    "    (\"DimTiempo\", [\"ANO_EJE\", \"MES_EJE\"], \"id_tiempo\"),\n",
    "    (\"DimUbicacion\", [\"DEPARTAMENTO_EJECUTORA_NOMBRE\", \"PROVINCIA_EJECUTORA_NOMBRE\", \"DISTRITO_EJECUTORA_NOMBRE\"], \"id_ubicacion\"),\n",
    "    (\"DimEntidad\", [\"NIVEL_GOBIERNO_NOMBRE\", \"SECTOR_NOMBRE\", \"PLIEGO_NOMBRE\", \"EJECUTORA_NOMBRE\"], \"id_entidad\"),\n",
    "    (\"DimActividad\", [\"PROGRAMA_PPTO_NOMBRE\", \"TIPO_ACT_PROY_NOMBRE\", \"PRODUCTO_PROYECTO_NOMBRE\", \"ACTIVIDAD_ACCION_OBRA_NOMBRE\"], \"id_actividad\")\n",
    "]\n",
    "\n",
    "# Ejecutar la función para cada dimensión\n",
    "for table_name, columns, id_col in dim_tables:\n",
    "    table_path = f\"Tables/{table_name}\"\n",
    "    create_or_update_dim_table(df, columns, id_col, table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dafa83d-2fc6-4912-b5c5-f4ef08857890",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-22T00:01:33.6852328Z",
       "execution_start_time": "2025-03-22T00:01:23.6414086Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "bbc399ee-7127-4b5f-8880-0988ced4422c",
       "queued_time": "2025-03-22T00:00:26.9964812Z",
       "session_id": "d4a5bc0d-7cf5-4ec8-8634-dbbe7d1b5f1c",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 9,
       "statement_ids": [
        9
       ]
      },
      "text/plain": [
       "StatementMeta(, d4a5bc0d-7cf5-4ec8-8634-dbbe7d1b5f1c, 9, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Historial de versiones de la tabla Hechos antes de la actualización:\n",
      "+-------+--------------------+---------+\n",
      "|version|           timestamp|operation|\n",
      "+-------+--------------------+---------+\n",
      "|      2|2025-03-21 23:50:...|    WRITE|\n",
      "|      1|2025-03-21 23:50:...|   DELETE|\n",
      "|      0|2025-03-21 21:51:...|    WRITE|\n",
      "+-------+--------------------+---------+\n",
      "\n",
      "📌 Eliminando registros de los años 2023, 2024 y 2025 en Hechos...\n",
      "📌 Eliminando registros con id_tiempo en: [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "✅ Registros antiguos eliminados.\n",
      "✅ Tabla de hechos actualizada correctamente para los años 2023, 2024 y 2025.\n",
      "📌 Historial de versiones de la tabla Hechos después de la actualización:\n",
      "+-------+--------------------+---------+\n",
      "|version|           timestamp|operation|\n",
      "+-------+--------------------+---------+\n",
      "|      4|2025-03-22 00:01:...|    WRITE|\n",
      "|      3|2025-03-22 00:01:...|   DELETE|\n",
      "|      2|2025-03-21 23:50:...|    WRITE|\n",
      "|      1|2025-03-21 23:50:...|   DELETE|\n",
      "|      0|2025-03-21 21:51:...|    WRITE|\n",
      "+-------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificar si la tabla Hechos ya existe\n",
    "try:\n",
    "    hechos_delta = DeltaTable.forPath(spark, ruta_hechos)\n",
    "    tabla_existe = True\n",
    "except AnalysisException:\n",
    "    print(\"⚠️ La tabla Hechos aún no existe. Se creará por primera vez.\")\n",
    "    tabla_existe = False\n",
    "\n",
    "# Si la tabla YA EXISTE: eliminar datos de los años 2023, 2024, 2025 y volver a insertar\n",
    "if tabla_existe:\n",
    "    print(\"📌 Historial de versiones de la tabla Hechos antes de la actualización:\")\n",
    "    hechos_delta.history().select(\"version\", \"timestamp\", \"operation\").show(5)\n",
    "\n",
    "    print(\"📌 Eliminando registros de los años 2023, 2024 y 2025 en Hechos...\")\n",
    "\n",
    "    # Cargar DimTiempo para obtener los id_tiempo de los años a actualizar\n",
    "    dim_tiempo = spark.read.format(\"delta\").load(\"Tables/DimTiempo\")\n",
    "    id_tiempos_actualizar = dim_tiempo.filter(col(\"ANO_EJE\").isin(ANOS_ACTUALIZAR)).select(\"id_tiempo\")\n",
    "\n",
    "    # Convertir los IDs a una lista para eliminar en Delta Table\n",
    "    id_tiempo_lista = [row[\"id_tiempo\"] for row in id_tiempos_actualizar.collect()]\n",
    "\n",
    "    if id_tiempo_lista:\n",
    "        print(f\"📌 Eliminando registros con id_tiempo en: {id_tiempo_lista}\")\n",
    "        hechos_delta.delete(col(\"id_tiempo\").isin(id_tiempo_lista))\n",
    "        print(\"✅ Registros antiguos eliminados.\")\n",
    "\n",
    "        # Insertar nueva data filtrada solo para los años a actualizar\n",
    "        df_hechos_nuevo = df.filter(col(\"ANO_EJE\").isin(ANOS_ACTUALIZAR)) \\\n",
    "            .join(dim_tiempo, [\"ANO_EJE\", \"MES_EJE\"], \"left\") \\\n",
    "            .join(spark.read.format(\"delta\").load(\"Tables/DimUbicacion\"), [\"DEPARTAMENTO_EJECUTORA_NOMBRE\", \"PROVINCIA_EJECUTORA_NOMBRE\", \"DISTRITO_EJECUTORA_NOMBRE\"], \"left\") \\\n",
    "            .join(spark.read.format(\"delta\").load(\"Tables/DimEntidad\"), [\"NIVEL_GOBIERNO_NOMBRE\", \"SECTOR_NOMBRE\", \"PLIEGO_NOMBRE\", \"EJECUTORA_NOMBRE\"], \"left\") \\\n",
    "            .join(spark.read.format(\"delta\").load(\"Tables/DimActividad\"), [\"PROGRAMA_PPTO_NOMBRE\", \"TIPO_ACT_PROY_NOMBRE\", \"PRODUCTO_PROYECTO_NOMBRE\", \"ACTIVIDAD_ACCION_OBRA_NOMBRE\"], \"left\") \\\n",
    "            .select(\n",
    "                \"id_tiempo\", \"id_ubicacion\", \"id_entidad\", \"id_actividad\",\n",
    "                \"MONTO_PIA\", \"MONTO_PIM\", \"MONTO_CERTIFICADO\", \"MONTO_COMPROMETIDO\",\n",
    "                \"MONTO_DEVENGADO\", \"MONTO_GIRADO\"\n",
    "            )\n",
    "\n",
    "        # Insertar registros nuevos en la tabla Hechos\n",
    "        df_hechos_nuevo.write.format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .partitionBy(\"id_tiempo\") \\\n",
    "            .save(ruta_hechos)\n",
    "\n",
    "        print(\"✅ Tabla de hechos actualizada correctamente para los años 2023, 2024 y 2025.\")\n",
    "        \n",
    "        # Ver historial después de la actualización\n",
    "        print(\"📌 Historial de versiones de la tabla Hechos después de la actualización:\")\n",
    "        hechos_delta.history().select(\"version\", \"timestamp\", \"operation\").show(5)\n",
    "    else:\n",
    "        print(\"⚠️ No se encontraron id_tiempo para los años seleccionados.\")\n",
    "\n",
    "# Si la tabla NO EXISTE: Crear desde cero\n",
    "else:\n",
    "    print(\"📌 Creando la tabla Hechos por primera vez...\")\n",
    "\n",
    "   # Unir IDs con la tabla de hechos\n",
    "    df_hechos = df \\\n",
    "        .join(spark.read.format(\"delta\").load(\"Tables/DimTiempo\"), [\"ANO_EJE\", \"MES_EJE\"], \"left\") \\\n",
    "        .join(spark.read.format(\"delta\").load(\"Tables/DimUbicacion\"), [\"DEPARTAMENTO_EJECUTORA_NOMBRE\", \"PROVINCIA_EJECUTORA_NOMBRE\", \"DISTRITO_EJECUTORA_NOMBRE\"], \"left\") \\\n",
    "        .join(spark.read.format(\"delta\").load(\"Tables/DimEntidad\"), [\"NIVEL_GOBIERNO_NOMBRE\", \"SECTOR_NOMBRE\", \"PLIEGO_NOMBRE\", \"EJECUTORA_NOMBRE\"], \"left\") \\\n",
    "        .join(spark.read.format(\"delta\").load(\"Tables/DimActividad\"), [\"PROGRAMA_PPTO_NOMBRE\", \"TIPO_ACT_PROY_NOMBRE\", \"PRODUCTO_PROYECTO_NOMBRE\", \"ACTIVIDAD_ACCION_OBRA_NOMBRE\"], \"left\") \\\n",
    "        .select(\n",
    "            \"id_tiempo\", \"id_ubicacion\", \"id_entidad\", \"id_actividad\",\n",
    "            \"MONTO_PIA\", \"MONTO_PIM\", \"MONTO_CERTIFICADO\", \"MONTO_COMPROMETIDO\",\n",
    "            \"MONTO_DEVENGADO\", \"MONTO_GIRADO\"\n",
    "        )\n",
    "\n",
    "\n",
    "    # Guardar en Delta optimizado\n",
    "    df_hechos.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"id_tiempo\") \\\n",
    "        .save(ruta_hechos)\n",
    "\n",
    "    print(\"✅ Tabla de hechos creada por primera vez.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ca51bc2-056d-4b5f-adc0-18fdf5ee9633",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-03-22T00:04:39.9611862Z",
       "execution_start_time": "2025-03-22T00:04:37.6516591Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "553dbdca-9fb9-4d1e-9fa8-da048e07ef0e",
       "queued_time": "2025-03-22T00:04:37.649996Z",
       "session_id": "d4a5bc0d-7cf5-4ec8-8634-dbbe7d1b5f1c",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 14,
       "statement_ids": [
        14
       ]
      },
      "text/plain": [
       "StatementMeta(, d4a5bc0d-7cf5-4ec8-8634-dbbe7d1b5f1c, 14, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fecha de actualización registrada correctamente: 2025-03-21 19:04:37\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Obtener la fecha y hora actual en UTC\n",
    "utc_now = datetime.utcnow()\n",
    "\n",
    "# Convertir a la zona horaria de Lima (UTC-5)\n",
    "peru_tz = pytz.timezone(\"America/Lima\")\n",
    "fecha_actualizacion = utc_now.replace(tzinfo=pytz.utc).astimezone(peru_tz).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Crear DataFrame con la fecha de actualización\n",
    "df_update = spark.createDataFrame([(fecha_actualizacion,)], [\"fecha_actualizacion\"])\n",
    "\n",
    "# Guardar en el Lakehouse en la ruta Tables/update_log (sobrescribe con cada ejecución)\n",
    "df_update.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/update_log\")\n",
    "\n",
    "print(\"✅ Fecha de actualización registrada correctamente:\", fecha_actualizacion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce503a-7c34-487f-bd9a-2e267ef1f210",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "cbd9d87f-ef6f-4221-8791-ee9d631dc83e",
    "default_lakehouse_name": "Gasto_Covid_Lakehouse",
    "default_lakehouse_workspace_id": "fed4e63f-0de2-47ef-84dd-06afe7c10427"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "es"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
